{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "어간 추출(Stemming) and 표제어 추출(Lemmatization).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOQ3g+PFx3dR7EY/P5rRZ4g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gksrbans/NLP-Wiki/blob/main/%EC%96%B4%EA%B0%84_%EC%B6%94%EC%B6%9C(Stemming)_and_%ED%91%9C%EC%A0%9C%EC%96%B4_%EC%B6%94%EC%B6%9C(Lemmatization).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_p9nHnaFLti"
      },
      "source": [
        "# 정규화 기법중 코퍼스내 단어 데이터를 줄일 수 있는 방법인 '표제어 추출' 과 '어간 추출'\n",
        "# => 서로 단어이지만 의미상 같다면 하나의 단어로 일반화 시킴.\n",
        "\n",
        "# 자연어 처리에서 정규화는 코퍼스의 복잡성을 줄이는 일에서 시작함.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eq5zJjfWJ-1M",
        "outputId": "33b558e3-3f36-4178-bd4a-5c0aa18813b1"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFtZdlNLFufu",
        "outputId": "47585fa6-7a2a-43f3-a19b-8e1ccb39fd9d"
      },
      "source": [
        "# 1. 표제어 추출 WordNetLemmaizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "print([lemmatizer.lemmatize(w) for w in words])\n",
        "\n",
        "# 단어의 형태가 완전히 보전되지 못함."
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "sX5ojORCKaGl",
        "outputId": "53d71d9e-5044-4a99-a231-eb07fbcbc09c"
      },
      "source": [
        "# 표제어 추출기의 경우 품사의 정보를 입력해주면 정확한 결과를 얻을 수 있음.\n",
        "lemmatizer.lemmatize('dies','v')\n",
        "lemmatizer.lemmatize('watched','v')\n",
        "lemmatizer.lemmatize('has', 'v')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'have'"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwAasA8VLi0V",
        "outputId": "bca56fed-ccaa-4116-b3f8-9ad5b6e5a88b"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1U3j8_SLFo7",
        "outputId": "21eacaf8-66bb-4f54-fca2-cc46af918ca6"
      },
      "source": [
        "# 2. 어간 추출(stemming)\n",
        "\n",
        "# Porter Algorithm - 정해진 규칙에 의해 문자를 제거함\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "text = \"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n",
        "words = word_tokenize(text)\n",
        "print(words)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOCVARUmLwfl",
        "outputId": "982e734e-3747-4e88-c320-858dd2795bef"
      },
      "source": [
        "print([stemmer.stem(w) for w in words])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "zf8m0tjdOnFN",
        "outputId": "be894edb-e54d-4634-d148-4d4a9dd8e2e4"
      },
      "source": [
        "words = ['formalize', 'allowance', 'electricical']\n",
        "print([stemmer.stem(w) for w in words])\n",
        "\n",
        "'''\n",
        "ALIZE → AL\n",
        "ANCE → 제거\n",
        "ICAL → IC\n",
        "'''"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['formal', 'allow', 'electric']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nALIZE → AL\\nANCE → 제거\\nICAL → IC\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xLPynIgPG9o",
        "outputId": "690bcc8a-85e7-4961-fce1-c97fcbc87154"
      },
      "source": [
        "# 랭커스터 스태머\n",
        "from nltk.stem import LancasterStemmer\n",
        "lancaster_stemmer = LancasterStemmer()\n",
        "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "print([lancaster_stemmer.stem(w) for w in words])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwuKqVRYFt4O"
      },
      "source": [
        ""
      ]
    }
  ]
}
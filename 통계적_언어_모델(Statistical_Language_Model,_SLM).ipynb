{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "통계적 언어 모델(Statistical Language Model, SLM).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMaoQeTW3vKtuFg4WwiUCsq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gksrbans/NLP-Wiki/blob/main/%ED%86%B5%EA%B3%84%EC%A0%81_%EC%96%B8%EC%96%B4_%EB%AA%A8%EB%8D%B8(Statistical_Language_Model%2C_SLM).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZ30xv5PAOaV"
      },
      "source": [
        "# 언어 모델.\n",
        "# 언어 모델이란 단어 또는 문장 시퀀스에 확률을 할당.\n",
        "\n",
        "# 언어모델을 만드는 방법은 통계와 인공신경망이 있는데 인공신경망이 더 자주쓰임 (GPT, BERT)\n",
        "# 통계적인 방법만 먼저보면, 이전 단어들이 주어졌을 때 다음 단어를 예측하는 방법이 보편적임.\n",
        "\n",
        "# 주어진 양쪽의 단어들로부터 가운데 비어있는 단어를 예측하는 언어 모델도 있음.\n",
        "# (이 유형의 언어 모델은 맨 마지막 BERT 챕터)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNLrZZDZILoP"
      },
      "source": [
        "# 문장에 대한 확률\n",
        "P(Hankyumoon he is good student) =\n",
        "P(Hankyumoon) X P(he|Hankyumoon) X P(is|Hankyumoon he) X P(good|Hankyumoon he is) X P(student|Hankyumoon he is good)\n",
        "이런식으로 문장에 대한 확률은 단어확률의 곱셈으로 나타낼 수 있다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_s4aLMw5IK4c"
      },
      "source": [
        "충분한 데이터를 관측하지 못하여 언어를 정확히 모델링하지 못하는 문제를 희소 문제(sparsity problem)라고 합니다.\n",
        "-> 분모가 0이되어 확률이란게 존재하지 않지만 문법적으로 적합하고 정답일 가능성이 있는 문제\n",
        "\n",
        "이러한 문제를 완화하는 방법으로 n-gram 또는 스무딩 백오프 등의 여러가지 일반화 기법이 존재함.\n",
        "-> 이러한 한계로 인해 언어모델의 트렌드가 통계에서 인공신경망을 넘어가게 되었음."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}